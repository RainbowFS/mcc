#!/usr/bin/env python3

import logging
from mcr.libmcr import g5k, ApiError
import sys
import time
from mcr.libsettings import *
import mcr.libsession
from mcr.libprint import print_items
import datetime
import dateutil.parser
import argparse
from mcr.libsalt import *
import threading

logging.basicConfig(stream=sys.stderr, level=logging.INFO)


def get_sites():
    return g5k(s)("stable/sites").get_items()


def find_site_for_cluster(cluster):
    for site in g5k(s)("stable/sites").get_items():
        if cluster in g5k(s)("stable/sites")(site)("clusters").get_items():
            return site
    raise ApiError(404, "Cluster not found")


def get_link_href(entity, rel="self"):
    for item in entity["links"]:

        if item["rel"] == rel:
            return item["href"]
    return ""


def find_job(job, sites_hints=None):
    return find_sub_item("jobs", int(job), sites_hints)


def find_dep(dep, sites_hints=None):
    return find_sub_item("deployments", dep, sites_hints)


def print_site_item(items_name, uid, sites, filter):
    kwargs = {splat[0]: splat[1] for splat in (item.split("=") for item in filter)}
    kwargs["user_uid"] = settings["login"]

    if opts.uid is None:
        for site in g5k(s)("stable")("sites").get_items():
            if sites is None or site in sites:
                return g5k(s)("stable")("sites")(site)(items_name).get_items_filtered(data=not opts.quiet, **kwargs)

    else:

        return [g5k(s)(find_sub_item(items_name, uid, sites)).get_raw()]


def find_sub_item(item, uid, sites_hints):
    '''

    :param job: job uid
    :param sites_hints: list of possible sites to look for. If list is empty of None, all g5k sites are inspected
    :return: the url of the job
    '''

    if sites_hints is None or len(sites_hints) == 0:
        target_sites = g5k(s)("stable")("sites").get_items()
    else:
        target_sites = sites_hints

    for site in target_sites:
        try:
            items_for_site = g5k(s)("stable")("sites")(site)(item)(uid).get_raw()
            return get_link_href(items_for_site, rel="self")
        except ApiError as e:
            if e.return_code == 404:
                continue
            else:
                raise e

    return ""


def get_wall_time(duraction_adv, duration):
    if duraction_adv == "for":
        dt = (dateutil.parser.parse(duration) - dateutil.parser.parse("0h"))

    elif duraction_adv == "until":
        dt = (dateutil.parser.parse(duration) - datetime.datetime.now())

    return "%02d:%02d" % ((dt.seconds // 3600), (dt.seconds // 60) % 60)


class ParseSettings(argparse.Action):

    def __init__(self, option_strings, dest, nargs=None, **kwargs):
        super(ParseSettings, self).__init__(option_strings, dest, **kwargs)

    def __call__(self, parser, namespace, value, option_string=None):
        k, v = value.split('=')
        namespace.settings[k] = v


parser = argparse.ArgumentParser()
subparsers = parser.add_subparsers(dest="command")
parser.add_argument("-q", help="quiet mode, just display the uids", action="store_true", dest="quiet")
parser.add_argument("--format", help="output formatting template, jinja", default=None)
parser.add_argument("--dry-run", help="just print the http requests", action="store_true", dest="dry")
parser.add_argument("--config",
                    help="path of the config file, by default mcc will look at a settings.yaml file in the current folder",
                    default=None)
parser.add_argument("-s", action=ParseSettings, help="override config file settings", nargs="*", default={},
                    dest="settings")

job_parser = subparsers.add_parser("job")

action_job_parser = job_parser.add_subparsers(dest="action")

list_job_parser = action_job_parser.add_parser("list")
list_job_parser.add_argument("uid", type=int, nargs='?', help='uid of the job to inspect', default=None)
list_job_parser.add_argument("--sites", type=str, nargs='*', help='list of sites for job search',
                             default=None)
list_job_parser.add_argument("--filter", type=str, nargs='*', help='list of filters to job seach, e.g. status=running',
                             default=[])

hosts_list_job_parser = action_job_parser.add_parser("hosts-list")
hosts_list_job_parser.add_argument("uid", type=int, nargs='?', help='uid of the job for which to show the hosts',
                                   default=None)
hosts_list_job_parser.add_argument("--site", type=str, nargs='?', help='hint of where the site job is',
                                   default=None)

add_job_parser = action_job_parser.add_parser("add")
add_job_parser.add_argument("site", type=str, help='site where to deploy the job')
add_job_parser.add_argument("node_count", type=int, help='how many node to book')
add_job_parser.add_argument("duration_adv", type=str, help='how many node to book', choices=["until", 'for'],
                            default="for")
add_job_parser.add_argument("duration", type=str, help='duration or expiration date for the job', default="1h")

wait_job_parser = action_job_parser.add_parser("wait")
wait_job_parser.add_argument("uid", type=str, help='uid of the job')
wait_job_parser.add_argument("--filter", type=str, help='wait until the condition is true e.g. state=running',
                             default="state=running")
wait_job_parser.add_argument("--site", type=str, nargs='?', help='hint of where the site job is',
                             default=None)

del_job_parser = action_job_parser.add_parser("del")
del_job_parser.add_argument("uid", type=str, help='uid of the job to delete')
del_job_parser.add_argument("--site", type=str, nargs='?', help='hint of where the site job is',
                            default=None)

install_job_parser = action_job_parser.add_parser("install")
install_job_parser.add_argument("uid", type=str, help='uid of the job to install to')
install_job_parser.add_argument("--site", type=str, nargs='?', help='hint of where the site job is',
                                default=None)

app_install_job_parser = install_job_parser.add_subparsers(dest="application")
salt_app_install_job_parser = app_install_job_parser.add_parser("salt")

dep_parser = subparsers.add_parser("dep")

action_dep_parser = dep_parser.add_subparsers(dest="action")

add_dep_parser = action_dep_parser.add_parser("add")
add_dep_parser.add_argument("uid", type=str, help='uid of the job on which to do the deployment')
add_dep_parser.add_argument("nodes", type=str, nargs='*',
                            help='names of the nodes on which to perform the deployement. all nodes from the job are deployed if ommited',
                            default=None)
add_dep_parser.add_argument("--site", type=str, nargs='?', help='hint of where the site job is',
                            default=None)
add_dep_parser.add_argument("--environment", type=str, help='name of the environment to install',
                            default="debian9-x64-base")

list_dep_parser = action_dep_parser.add_parser("list")
list_dep_parser.add_argument("uid", type=str, nargs='?', help='uid of the dep to inspect', default=None)
list_dep_parser.add_argument("--sites", type=str, nargs='*', help='list of sites for dep search')
list_dep_parser.add_argument("--filter", type=str, nargs='*',
                             help='list of filters to dep seach, e.g. state=running state!=error',
                             default=[])

list_dep_parser = action_dep_parser.add_parser("wait")
list_dep_parser.add_argument("uid", type=str, nargs='?', help='uid of the dep to inspect', default=None)
list_dep_parser.add_argument("--filter", type=str, nargs='*',
                             help='wait until the condition is true eg. status=terminate',
                             default="status=terminated")
list_dep_parser.add_argument("--site", type=str, nargs='?', help='hint of where the site job is',
                             default=None)

# Parse
opts = parser.parse_args()

settings = merge_settings(opts.config, opts.settings)

# default values

# can't have both -s default_site=... and --sites ...
if "default_site" in opts.settings and opts.__contains__("sites") and opts.sites is not None:
    print("Can't override both default site and possible sites")
    exit(1)

if opts.__contains__("site") and opts.site is None:
    opts.site = settings["default_site"]
if opts.__contains__("sites") and opts.sites is None:
    opts.sites = [settings["default_site"]]

# create API session
s = mcr.libsession.create_session(settings["api-backend"], settings["login"], settings["pwd"])


class Parsing():

    def __init__(self, opts):
        self.opts = opts

    def run(self):

        switch = {"job": self.handle_job, "dep": self.handle_dep}
        switch[self.opts.command]()

    def handle_job(self):

        switch = {"list": self.job_list_print,
                  "add": self.job_add,
                  "del": self.job_del,
                  "wait": self.job_wait,
                  "hosts-list": self.job_host_list_print,
                  "install": self.job_install
                  }

        switch[self.opts.action]()

    def handle_dep(self):
        switch = {"add": self.dep_add,
                  "list": self.dep_list,
                  "wait": self.dep_wait}

        switch[self.opts.action]()

    def job_install(self):
        if self.opts.application == "salt":
            threads = []

            for i, host in enumerate(self.job_host_list()):
                if i == 0:
                    master_ip = get_ip(host, settings["ssh_key_file_private"], settings["salt_master_interface"])

                    print("master ip: %s" % master_ip)
                    print("installing master in %s" % host)

                    t = threading.Thread(target=install_salt_master,
                                         args=(
                                             host, settings["ssh_key_file_private"], "h0", master_ip, settings))
                    t.start()
                    threads.append(t)
                else:

                    print("installing minion in %s" % host)
                    t = threading.Thread(target=install_salt_minion,
                                         args=(
                                             host, settings["ssh_key_file_private"], "h%s" % i, master_ip, settings))
                    t.start()
                    threads.append(t)
            for t in threads:
                t.join()
            print("done")

    def job_list(self):
        res = print_site_item("jobs", self.opts.uid, self.opts.sites, self.opts.filter)
        return res

    def job_list_print(self):
        res = self.job_list()
        print_items(res, self.opts.format)

    def job_add(self):
        wt = get_wall_time(self.opts.duration_adv, self.opts.duration)
        properties = []
        resources = []
        resources.append(("nodes", self.opts.node_count))
        resources.append(("walltime", wt))
        if self.opts.site in get_sites():
            site = self.opts.site
        else:
            properties.append(("cluster", "'%s'" % self.opts.site))
            site = find_site_for_cluster(self.opts.site)
        job_uid = g5k(s)("stable")("sites")(site).post_job(resources=resources, properties=properties)
        print(job_uid)

    def job_del(self):
        job_href = find_job(self.opts.uid, None if self.opts.site is None else [self.opts.site])
        g5k(s)(job_href).delete()
        logging.info("Job %s has been deleted " % job_href)

    def job_wait(self):
        job_href = find_job(self.opts.uid, [self.opts.site])
        k, v = self.opts.filter.split("=")
        job = g5k(s)(job_href).get_raw()
        while job[k] != v:

            if not self.opts.quiet:
                minutes_remaining = (int(job["scheduled_at"]) - int(time.time())) // 60
                sys.stdout.write(
                    "\b" * 80 + " %s minutes remaining (is %s)" % (minutes_remaining, job["state"]))
                sys.stdout.flush()
            time.sleep(5)
            job = g5k(s)(job_href).get_raw()

    def job_host_list_print(self):
        hosts = self.job_host_list()
        print("\n".join(hosts))

    def job_host_list(self):
        job_href = find_job(self.opts.uid, [self.opts.site])
        job = g5k(s)(job_href).get_raw()
        if job["state"] == "running":
            return job["assigned_nodes"]
        else:
            raise Exception("Cannot show hosts, job is %s " % job["state"])

    def dep_add(self):
        job = g5k(s)(find_job(self.opts.uid, None if self.opts.site is None else [self.opts.site])).get_raw()
        if len(self.opts.nodes) == 0:
            node_list = job["assigned_nodes"]
        else:
            node_list = list(set(job["assigned_nodes"]) & set(self.opts.nodes))
        dep_uid = g5k(s)(get_link_href(job, "parent"))("deployments").post_provision(node_list=node_list,
                                                                                     key=settings["ssh_key"],
                                                                                     environment=self.opts.environment)
        print(dep_uid)

    def dep_list(self):
        res = print_site_item("deployments", self.opts.uid, self.opts.sites, self.opts.filter)
        print_items(res, self.opts.format)

    def dep_wait(self):
        dep_href = find_dep(self.opts.uid, [self.opts.site])
        k, v = self.opts.filter.split("=")
        dep = g5k(s)(dep_href).get_raw()
        while dep[k] != v:
            if not self.opts.quiet:
                minutes_elapsed = (int(time.time()) - int(dep["created_at"])) // 60
                sys.stdout.write(
                    "\b" * 80 + "%s minutes elapsed (is %s)" % (minutes_elapsed, dep["status"]))
                sys.stdout.flush()
            time.sleep(10)
            dep = g5k(s)(dep_href).get_raw()


try:

    Parsing(opts).run()
    exit(0)

except KeyboardInterrupt:
    pass
